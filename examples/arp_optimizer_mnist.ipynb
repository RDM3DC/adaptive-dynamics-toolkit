{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3270ca3",
   "metadata": {},
   "source": [
    "# MNIST Classification with ARP Optimizer\n",
    "\n",
    "This notebook demonstrates using the Adaptive Resistance-Potential (ARP) optimizer for training a neural network on the MNIST handwritten digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676934d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Import the ARP optimizer\n",
    "from adaptive_dynamics.arp.optimizers import ARP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e084dc",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a320e91",
   "metadata": {},
   "source": [
    "## 2. Define Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f340edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d9059",
   "metadata": {},
   "source": [
    "## 3. Set Up Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a43fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create ARP optimizer\n",
    "optimizer = ARP(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,        # Learning rate\n",
    "    alpha=0.01,      # Adaptation rate\n",
    "    mu=0.001,       # Decay rate\n",
    "    weight_decay=1e-5  # L2 regularization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6f24eb",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78951521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights with ARP\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Print progress\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\t'\n",
    "                  f'Loss: {loss.item():.6f}\\t'\n",
    "                  f'Accuracy: {100. * correct / total:.2f}%')\n",
    "    \n",
    "    # Return average loss and accuracy for the epoch\n",
    "    return train_loss / len(train_loader), correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({100. * accuracy:.2f}%)\\n')\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc044fad",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Training loop\n",
    "start_time = time()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "training_time = time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de42f2ff",
   "metadata": {},
   "source": [
    "## 6. Plot Training and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, 'bo-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), test_losses, 'ro-', label='Test Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), [acc * 100 for acc in train_accuracies], 'bo-', label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), [acc * 100 for acc in test_accuracies], 'ro-', label='Test Accuracy')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701178d",
   "metadata": {},
   "source": [
    "## 7. Visualize ARP's Adaptive Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541008ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get adaptive learning rates\n",
    "adaptive_lr_dict = optimizer.get_adaptive_lr()\n",
    "\n",
    "# Collect learning rates for each layer\n",
    "layer_names = []\n",
    "avg_learning_rates = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param in adaptive_lr_dict:\n",
    "        lr_tensor = adaptive_lr_dict[param]\n",
    "        avg_lr = lr_tensor.mean().item()\n",
    "        layer_names.append(name)\n",
    "        avg_learning_rates.append(avg_lr)\n",
    "\n",
    "# Plot average learning rate for each layer\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(layer_names, avg_learning_rates)\n",
    "plt.xlabel('Average Effective Learning Rate')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('ARP: Adaptive Learning Rates by Layer')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8846c761",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions on Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot images with predictions\n",
    "def plot_predictions(model, device, test_loader, num_images=10):\n",
    "    model.eval()\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "    # Plot images\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[i].cpu().numpy().squeeze(), cmap='gray')\n",
    "        title = f\"Pred: {predicted[i].item()}\\nTrue: {labels[i].item()}\"\n",
    "        color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "        plt.title(title, color=color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "plot_predictions(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34d8c8",
   "metadata": {},
   "source": [
    "## 9. Compare with Standard Optimizers (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_compare_optimizers(opt_name, optimizer, epochs=3):\n",
    "    # Create a new model\n",
    "    model = SimpleNN().to(device)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Training loop\n",
    "    start_time = time()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\n{opt_name} - Epoch {epoch}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, _ = train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        _, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    training_time = time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return train_losses, test_accuracies, training_time\n",
    "\n",
    "# Compare ARP with Adam and SGD\n",
    "# Uncomment to run this comparison\n",
    "\"\"\"\n",
    "print(\"Training with ARP optimizer\")\n",
    "arp_opt = ARP(SimpleNN().parameters(), lr=1e-3, alpha=0.01, mu=0.001)\n",
    "arp_losses, arp_accs, arp_time = train_compare_optimizers(\"ARP\", arp_opt)\n",
    "\n",
    "print(\"Training with Adam optimizer\")\n",
    "adam_opt = torch.optim.Adam(SimpleNN().parameters(), lr=1e-3)\n",
    "adam_losses, adam_accs, adam_time = train_compare_optimizers(\"Adam\", adam_opt)\n",
    "\n",
    "print(\"Training with SGD optimizer\")\n",
    "sgd_opt = torch.optim.SGD(SimpleNN().parameters(), lr=0.01, momentum=0.9)\n",
    "sgd_losses, sgd_accs, sgd_time = train_compare_optimizers(\"SGD\", sgd_opt)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(arp_losses) + 1), arp_losses, 'b-', label=f'ARP ({arp_time:.2f}s)')\n",
    "plt.plot(range(1, len(adam_losses) + 1), adam_losses, 'r-', label=f'Adam ({adam_time:.2f}s)')\n",
    "plt.plot(range(1, len(sgd_losses) + 1), sgd_losses, 'g-', label=f'SGD ({sgd_time:.2f}s)')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(arp_accs) + 1), [acc * 100 for acc in arp_accs], 'b-', label='ARP')\n",
    "plt.plot(range(1, len(adam_accs) + 1), [acc * 100 for acc in adam_accs], 'r-', label='Adam')\n",
    "plt.plot(range(1, len(sgd_accs) + 1), [acc * 100 for acc in sgd_accs], 'g-', label='SGD')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844d0b9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated using the ARP optimizer from the Adaptive Dynamics Toolkit to train a neural network on the MNIST dataset. The ARP optimizer adapts learning rates based on gradient history using a conductance-like mechanism, potentially providing benefits over traditional optimizers in certain scenarios.\n",
    "\n",
    "Key takeaways:\n",
    "1. The ARP optimizer is easy to use as a drop-in replacement for standard optimizers\n",
    "2. Its adaptive learning rate mechanism allows it to adjust to the training dynamics\n",
    "3. The conductance state provides a form of memory similar to momentum but based on different principles\n",
    "\n",
    "For more advanced use cases, you can experiment with different values for the adaptation rate (alpha) and decay rate (mu) parameters to fine-tune the optimizer's behavior for your specific problem."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
